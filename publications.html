<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
<meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff">

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha256-CylD++oS3IkJmSs/j4i1w0cv2wpVQRLLQqw/m+Fvxts=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Publications | William Seymour</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Publications" />
<meta name="author" content="William Seymour" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="List of publications" />
<meta property="og:description" content="List of publications" />
<meta property="og:site_name" content="William Seymour" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"William Seymour"},"description":"List of publications","headline":"Publications","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/img/profile.webp"},"name":"William Seymour"},"url":"/publications.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <a href="/">
          <li class="btn-nav">Home</li>
        </a>
        <li class="current">Publications</li>
        
        
        <!-- presentations -->
        
      </ul>
    </nav>
  </header>
</section>
<!--publications.js-->
<script type="text/javascript" src="/assets/js/publications.js"></script>
<div id="publications">
  <section class="bg"></section>
  <h1 class="title">Publications</h1>
  <h2 class="bibliography">2023</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="10.1145/3313831.3376264">
  
  <!-- Title -->
  <div class="title">Legal Obligation and Ethical Best Practice: Towards Meaningful Verbal Consent for Voice Assistants</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;Cote, Mark,&nbsp;and Such, Jose
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pp. 1–16, 2023
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CHI23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3544548.3580967" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>To improve user experience, Alexa now allows users to consent to data sharing via voice rather than directing them to the companion smartphone app. While verbal consent mechanisms for voice assistants (VAs) can increase usability, they can also undermine principles core to informed consent. We conducted a Delphi study with experts from academia, industry, and the public sector on requirements for verbal consent in VAs. Candidate requirements were drawn from the literature, regulations, and research ethics guidelines that participants rated based on their relevance to the consent process, actionability by platforms, and usability by end-users, discussing their reasoning as the study progressed. We highlight key areas of (dis)agreement between experts, deriving recommendations for regulators, skill developers, and VA platforms towards crafting meaningful verbal consent mechanisms. Key themes include approaching permissions according to the user’s ability to opt-out, minimising consent decisions, and ensuring platforms follow established consent principles.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3579497">
  
  <!-- Title -->
  <div class="title">Ignorance is Bliss? The Effect of Explanations on Perceptions of Voice Assistants</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;and Such, Jose
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    Proc. ACM Hum.-Comput. Interact., vol. 7, Apr, 2023
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CSCW23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3579497" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Voice assistants offer a convenient and hands-free way of accessing computing in the home, but a key problem with speech as an interaction modality is how to scaffold accurate mental models of voice assistants, a task complicated by privacy and security concerns. We present the results of a survey of voice assistant users (n=1314) measuring trust, security, and privacy perceptions of voice assistants with varying levels of online functionality explained in different ways. We then asked participants to re-explain how these voice assistants worked, showing that while privacy explanations relieved privacy concerns, trust concerns were exacerbated by trust explanations. Participants’ trust, privacy, and security perceptions also distinguished between first party online functionality from the voice assistant vendor and third party online functionality from other developers, and trust in vendors appeared to operate independently from device explanations. Our findings point to the use of analogies to guide users, targeting trust and privacy concerns, key improvements required from manufacturers, and implications for competition in the sector.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="10.1145/3543829.3544521">
  
  <!-- Title -->
  <div class="title">Can You Meaningfully Consent in Eight Seconds? Identifying Ethical Issues with Verbal Consent for Voice Assistants</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;Cote, Mark,&nbsp;and Such, Jose
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 4th Conference on Conversational User Interfaces, 2022
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CUI22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3543829.3544521" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Determining how voice assistants should broker consent to share data with third party software has proven to be a complex problem. Devices often require users to switch to companion smartphone apps in order to navigate permissions menus for their otherwise hands-free voice assistant. More in line with smartphone app stores, Alexa now offers “voice-forward consent”, allowing users to grant skills access to personal data mid-conversation using speech. While more usable and convenient than opening a companion app, asking for consent ‘on the fly’ can undermine several concepts core to the informed consent process. The intangible nature of voice interfaces further blurs the boundary between parts of an interaction controlled by third-party developers from the underlying platforms. This provocation paper highlights key issues with current verbal consent implementations, outlines directions for potential solutions, and presents five open questions to the research community. In so doing, we hope to help shape the development of usable and effective verbal consent for voice assistants and similar conversational user interfaces.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3543829.3544513">
  
  <!-- Title -->
  <div class="title">When It’s Not Worth the Paper It’s Written On: A Provocation on the Certification of Skills in the Alexa and Google Assistant Ecosystems</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;Cote, Mark,&nbsp;and Such, Jose
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 4th Conference on Conversational User Interfaces, 2022
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CUI22b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3543829.3544513" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The increasing reach and functionality of voice assistants has allowed them to become a general-purpose platform for tasks like playing music, accessing information, and controlling smart home devices. In order to maintain the quality of third-party skills and to protect children and other members of the public from inappropriate or malicious skills, platform providers have developed content policies and certification procedures that skills must undergo prior to public release. Unfortunately, research suggests that these measures have been ineffective at curating voice assistant platforms, with documented instances of skills with significant security and privacy problems. This provocation paper outlines how the underlying architectures of these platforms had turned skill certification into a seemingly intractable problem, as well as how current certification methods fall short of their full potential. We present a roadmap for improving the state of skill certification on contemporary voice assistant platforms, including research directions and actions that need to be taken by platform vendors. Promoting this change in domestic voice assistants is especially important, as developers of commercial and industrial assistants or other similar contexts increasingly look to these devices for norms and conventions.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3514094.3534186">
  
  <!-- Title -->
  <div class="title">Respect as a Lens for the Design of AI Systems</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;Van Kleek, Max,&nbsp;Binns, Reuben,&nbsp;and Murray-Rust, Dave
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, pp. 641–652, 2022
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/AIES22.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3514094.3534186" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or ’safe’, it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn’t technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="10.1145/3479515">
  
  <!-- Title -->
  <div class="title">Exploring Interactions Between Trust, Anthropomorphism, and Relationship Development in Voice Assistants</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;and Van Kleek, Max
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    Proc. ACM Hum.-Comput. Interact., vol. 5, Oct, 2021
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CSCW21.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3479515" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Modern conversational agents such as Alexa and Google Assistant represent significant progress in speech recognition, natural language processing, and speech synthesis. But as these agents have grown more realistic, concerns have been raised over how their social nature might unconsciously shape our interactions with them. Through a survey of 500 voice assistant users, we explore whether users’ relationships with their voice assistants can be quantified using the same metrics as social, interpersonal relationships; as well as if this correlates with how much they trust their devices and the extent to which they anthropomorphise them. Using Knapp’s staircase model of human relationships, we find that not only can human-device interactions be modelled in this way, but also that relationship development with voice assistants correlates with increased trust and anthropomorphism.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="10.1145/3313831.3376265">
  
  <!-- Title -->
  <div class="title">Informing the Design of Privacy-Empowering Tools for the Connected Home</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;Kraemer, Martin J.,&nbsp;Binns, Reuben,&nbsp;and Van Kleek, Max
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–14, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CHI20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3313831.3376264" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Connected devices in the home represent a potentially grave new privacy threat due to their unfettered access to the most personal spaces in people’s lives. Prior work has shown that despite concerns about such devices, people often lack sufficient awareness, understanding, or means of taking effective action. To explore the potential for new tools that support such needs directly we developed Aretha, a privacy assistant technology probe that combines a network disaggregator, personal tutor, and firewall, to empower end-users with both the knowledge and mechanisms to control disclosures from their homes. We deployed Aretha in three households over six weeks, with the aim of understanding how this combination of capabilities might enable users to gain awareness of data disclosures by their devices, form educated privacy preferences, and to block unwanted data flows. The probe, with its novel affordances-and its limitations-prompted users to co-adapt, finding new control mechanisms and suggesting new approaches to address the challenge of regaining privacy in the connected home.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3313831.3376672">
  
  <!-- Title -->
  <div class="title">‘I Just Want to Hack Myself to Not Get Distracted’: Evaluating Design Interventions for Self-Control on Facebook</div>
  <!-- Author -->
  <div class="author">Lyngs, Ulrik,&nbsp;Lukoff, Kai,&nbsp;Slovak, Petr,&nbsp;Seymour, William,&nbsp;Webb, Helena,&nbsp;Jirotka, Marina,&nbsp;Zhao, Jun,&nbsp;Van Kleek, Max,&nbsp;and Shadbolt, Nigel
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–15, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/CHI20b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3313831.3376672" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Beyond being the world’s largest social network, Facebook is for many also one of its greatest sources of digital distraction. For students, problematic use has been associated with negative effects on academic achievement and general wellbeing. To understand what strategies could help users regain control, we investigated how simple interventions to the Facebook UI affect behaviour and perceived control. We assigned 58 university students to one of three interventions: goal reminders, removed newsfeed, or white background (control). We logged use for 6 weeks, applied interventions in the middle weeks, and administered fortnightly surveys. Both goal reminders and removed newsfeed helped participants stay on task and avoid distraction. However, goal reminders were often annoying, and removing the newsfeed made some fear missing out on information. Our findings point to future interventions such as controls for adjusting types and amount of available information, and flexible blocking which matches individual definitions of ’distraction’.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3334480.3381809">
  
  <!-- Title -->
  <div class="title">Does Siri Have a Soul? Exploring Voice Assistants Through Shinto Design Fictions</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;and Van Kleek, Max
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–12, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/ALTCHI20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3334480.3381809" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>It can be difficult to critically reflect on technology that has become part of everyday rituals and routines. To combat this, speculative and fictional approaches have previously been used by HCI to decontextualise the familiar and imagine alternatives. In this work we turn to Japanese Shinto narratives as a way to defamiliarise voice assistants, inspired by the similarities between how assistants appear to ’inhabit’ objects similarly to kami. Describing an alternate future where assistant presences live inside objects, this approach foregrounds some of the phenomenological quirks that can otherwise easily become lost. Divorced from the reality of daily life, this approach allows us to reevaluate some of the common interactions and design patterns that are common in the virtual assistants of the present.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3334480.3375032">
  
  <!-- Title -->
  <div class="title">A Design Philosophy for Agents in the Smart Home</div>
  <!-- Author -->
  <div class="author">Seymour, William
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1–9, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/DC20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3334480.3375032" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The home is often the most private space in people’s lives, and not one in which they expect to be surveilled. However, today’s market for smart home devices has quickly evolved to include products that monitor, automate, and present themselves as human. After documenting some of the more unusual emergent problems with contemporary devices, this body of work seeks to develop a design philosophy for intelligent agents in the smart home that can act as an alternative to the ways that these devices are currently built. This is then applied to the design of privacy empowering technologies, representing the first steps from the devices of the present towards a more respectful future.</p>
    </div>
</div></li>
<li><!-- Entry bib key -->
<div id="10.1145/3357236.3395501">
  
  <!-- Title -->
  <div class="title">Strangers in the Room: Unpacking Perceptions of ’Smartness’ and Related Ethical Concerns in the Home</div>
  <!-- Author -->
  <div class="author">Seymour, William,&nbsp;Binns, Reuben,&nbsp;Slovak, Petr,&nbsp;Van Kleek, Max,&nbsp;and Shadbolt, Nigel
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Proceedings of the 2020 ACM Designing Interactive Systems Conference, pp. 841–854, 2020
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/DIS20.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3357236.3395501" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The increasingly widespread use of ’smart’ devices has raised multifarious ethical concerns regarding their use in domestic spaces. Previous work examining such ethical dimensions has typically either involved empirical studies of concerns raised by specific devices and use contexts, or alternatively expounded on abstract concepts like autonomy, privacy or trust in relation to ’smart homes’ in general. This paper attempts to bridge these approaches by asking what features of smart devices users consider as rendering them ’smart’ and how these relate to ethical concerns. Through a multimethod investigation including surveys with smart device users (n=120) and semi-structured interviews (n=15), we identify and describe eight types of smartness and explore how they engender a variety of ethical concerns including privacy, autonomy, and disruption of the social order. We argue that this middle ground, between concerns arising from particular devices and more abstract ethical concepts, can better anticipate potential ethical concerns regarding smart devices.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="10.1145/3290607.3308449">
  
  <!-- Title -->
  <div class="title">Privacy Therapy with Aretha: What If Your Firewall Could Talk?</div>
  <!-- Author -->
  <div class="author">Seymour, William
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1–6, 2019
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/SRC19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3290607.3308449" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The rapid adoption of smart home devices has brought with it a widespread lack of understanding amongst users over where their devices send data. Smart home ecosystems represent complex additions to existing wicked problems around network privacy and security in the home. This work presents the Aretha project, a device which combines the functionality of a firewall with the position of voice assistants as the hub of the smart home, and the sophistication of modern conversational voice interfaces. The result is a device which can engage users in conversation about network privacy and security, allowing for the forming and development of complex preferences that Aretha is then able to act upon.</p>
    </div>
</div></li></ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li><!-- Entry bib key -->
<div id="10.1145/3170427.3180289">
  
  <!-- Title -->
  <div class="title">How Loyal is Your Alexa? Imagining a Respectful Smart Assistant</div>
  <!-- Author -->
  <div class="author">Seymour, William
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems, pp. 1–6, 2018
  </div>

  <!-- Links/Buttons -->
  <div class="links">
      [<a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>]
      [<a href="/assets/pdf/SRC18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>]
      [<a href="https://doi.org/10.1145/3170427.3180289" class="btn btn-sm z-depth-0" role="button">URL</a>]
  </div>

  <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Smart assistants are the current must-have device in the home. Currently available products do little to respect the autonomy and privacy of end users, but it doesn’t have to be this way. My research explores a speculative ’respectful’ assistant which is more socially aware, and treats its users in a more nuanced way than occurs at present. Mixing computer science, philosophy, and art, the project uses a combination of user studies and technical comparison to discover a potential future for the smart digital assistant.</p>
    </div>
</div></li></ol>
</div>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href=""></a>.
    Powered by Jekyll with
    <a href="https://github.com/chrjabs/Grape-Academic-Theme">Grape Academic Theme</a>.
  </div>
</footer>

  </div>
</body>

</html>